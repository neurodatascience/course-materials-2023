{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine-Learning tutorial for classification\n",
    "\n",
    "## Objective\n",
    "1. Predict diagnostic label from functional connectome\n",
    "2. Predict MRI acquisition site from functional connectome\n",
    "\n",
    "## Dataset\n",
    "[ABIDE (Autism)](https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_abide_pcp.html)\n",
    "\n",
    "## Preprocessing\n",
    "1. Generate region-to-region connectivity matrix (i.e. connectome) from functional timeseries data\n",
    "    - ROIs are defined based on [harvard_oxford atlas](https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_atlas_harvard_oxford.html) or [AAL atlas](https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_atlas_aal.html)\n",
    "2. Apply dimensionality reduction (Optional)\n",
    "\n",
    "## Model \n",
    "1. Logistic regression\n",
    "2. Random Forest\n",
    "\n",
    "## Cross-validation\n",
    "1. k-fold\n",
    "2. shuffle-split\n",
    "\n",
    "## post-hoc analysis\n",
    "Compare model performance for predicting Dx labels vs. MRI acquisition site\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's begin! \n",
    "### First we import some useful python libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhil/projects/green_comp_neuro/green_compute/lib/python3.7/site-packages/nilearn/datasets/__init__.py:89: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "from nilearn import datasets\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "### ROIs are defined based on [harvard_oxford atlas](https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_atlas_harvard_oxford.html) or [AAL atlas](https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_atlas_aal.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_subjects = 100\n",
    "parcel = \"rois_ho\"  # 'rois_ho' or 'rois_aal\n",
    "data = datasets.fetch_abide_pcp(n_subjects=n_subjects, derivatives=[parcel], data_dir=\"./\")  # 17:28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phenotypes and Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno = pd.DataFrame(data[\"phenotypic\"]).drop(columns=[\"i\", \"Unnamed: 0\"])\n",
    "pheno.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_counts = pheno[\"SITE_ID\"].value_counts()\n",
    "dx_counts = pheno[\"DX_GROUP\"].value_counts()\n",
    "\n",
    "print(f\"Dx count:\\n{dx_counts}\\n\\nScanning site_counts:\\n{site_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRI features\n",
    "\n",
    "### These are stored in a list, where each list element is a subject-specific feature matrix\n",
    "### Subject specific feature matrix: timepoints x ROIs\n",
    "### ROIs are defined based on [harvard_oxford atlas](https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_atlas_harvard_oxford.html) or [AAL atlas](https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_atlas_aal.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[parcel]\n",
    "\n",
    "print(f\"Number of samples: {len(features)}\")\n",
    "\n",
    "subject_feature_shape = features[0].shape\n",
    "\n",
    "print(f\"subject_feature_shape: {subject_feature_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how the atlas looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "\n",
    "if parcel == \"rois_ho\":\n",
    "    atlas = datasets.fetch_atlas_harvard_oxford(\"cort-maxprob-thr25-2mm\")\n",
    "else:\n",
    "    atlas = datasets.fetch_atlas_aal()\n",
    "\n",
    "plotting.plot_roi(atlas.maps, draw_cross=False, title=parcel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the subject-specific feature matrix\n",
    "### Here the feature set is organized in a (time x roi) matrix per individual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "g = sns.heatmap(features[0].T, ax=ax)\n",
    "g.set_xlabel(\"timepoint\")\n",
    "g.set_ylabel(\"ROI\")\n",
    "g.set_title(\"Functional data timeseries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing / feature engineering\n",
    "\n",
    "### Commonly functional (timeseries) neuroimaging data is represented as functional connectome aka network aka graph. \n",
    "### Here we convert the (time x roi) matrix into (roi x roi) matrix per individual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome_matrix = ConnectivityMeasure(kind=\"correlation\")\n",
    "connectome_matrix = connectome_matrix.fit_transform([features[0]])[0]\n",
    "print(f\"Shape connectome: {connectome_matrix.shape}\")\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "g = sns.heatmap(connectome_matrix, ax=ax)\n",
    "g.set_xlabel(\"ROI\")\n",
    "g.set_ylabel(\"ROI\")\n",
    "g.set_title(\"Connectome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract lower (or upper) triangle entrees (excluding diagonal)\n",
    "### Here we convert the (roi x roi) matrix into a single feature vector per individual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tril_idx = np.tril_indices(len(connectome_matrix), k=-1)\n",
    "features_flat = connectome_matrix[tril_idx]\n",
    "print(f\"Number of features per subject: {len(features_flat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now do this for all subjects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defs are definitely useful!\n",
    "\n",
    "\n",
    "def extract_connectome_features(func_data, measure):\n",
    "    \"\"\"A function to calculate connnectome based on timeseries data and similarity measure\"\"\"\n",
    "    # Build connectom matrix\n",
    "    # connectome_matrix =\n",
    "\n",
    "    # Extract lower triangular indices\n",
    "    # tril_idx =\n",
    "\n",
    "    # Grab the lower trianglar features\n",
    "    # flat_features =\n",
    "\n",
    "    return flat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: here we are pre-processing each image independently i.e. not using any group-level information for scaling / normalization / feature transformation (e.g. PCA). Therefore there is no \"double dipping\" or leakage of information from a test set. This sort of independent image pre-processing, we can do on entire dataset without creating train-test splits first and then defining feature-set on the training data only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_measure = ConnectivityMeasure(kind=\"correlation\")\n",
    "\n",
    "flat_features_list = []\n",
    "for func_data in features:\n",
    "    flat_features = extract_connectome_features(func_data, correlation_measure)\n",
    "    flat_features_list.append(flat_features)\n",
    "\n",
    "print(f\"Length of flat_features_list {len(flat_features_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data matrix (n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(flat_features_list)\n",
    "\n",
    "print(f\"Input data (X) shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output labels (y): Diagnosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "outcome = \"DX_GROUP\"\n",
    "y = pheno[outcome]\n",
    "y_counts = y.value_counts()\n",
    "\n",
    "print(f\"Unique output clasess:\\n{y_counts}\")\n",
    "\n",
    "# Encode labels to integer categories\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay now we have our input data (X) and output data (y) in the following format\n",
    "<img src=\"./QLS_ML_terminology.png\" alt=\"terms\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train-test split\n",
    "- 80/20 ratio\n",
    "- Stratify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_subset_fraction = 0.2  #\n",
    "stratification = y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,  # input features\n",
    "    y,  # output labels\n",
    "    test_size=test_subset_fraction,\n",
    "    shuffle=True,  # shuffle dataset\n",
    "    # before splitting\n",
    "    stratify=stratification,\n",
    "    random_state=123,  # same shuffle each time\n",
    ")\n",
    "\n",
    "# print the size of our training and test groups\n",
    "print(\"training:\", len(X_train), \"testing:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay finally, let's train your first model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = \"LR\"  # 'LR' or 'RF'\n",
    "\n",
    "if model == \"RF\":\n",
    "    clf = RandomForestClassifier(max_depth=3, class_weight=\"balanced\", random_state=0)\n",
    "elif model == \"LR\":\n",
    "    clf = LogisticRegression(penalty=\"l1\", C=1, class_weight=\"balanced\", random_state=0)\n",
    "else:\n",
    "    print(f\"Unknown model: {model}\")\n",
    "\n",
    "print(f\"Using model: {model}\")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "print(f\"train acc: {train_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test set\n",
    "- accuracy\n",
    "- confusion_matrix\n",
    "- precision_recall_fscore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(f\"test acc: {test_acc:.3f}\")\n",
    "\n",
    "test_cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Are we overfitting? \n",
    "\n",
    "## Q2: What can we do to mitigate overfitting? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "g = sns.heatmap(test_cm, ax=ax, annot=True, annot_kws={\"fontsize\": 15}, cmap=\"Reds\")\n",
    "g.set_title(\"Confusion matrix\")\n",
    "g.set_ylabel(\"True label\")\n",
    "g.set_xlabel(\"Pred label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(\n",
    "    f\"model: {model}, outcome: {outcome}\\n Acc:{test_acc:.2f}, precision: {p:.2f}, recall: {r:.2f}, f1: {f1:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try to predict scan site from the same features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = \"SITE_ID\"\n",
    "y = pheno[outcome]\n",
    "y_counts = y.value_counts()\n",
    "\n",
    "print(f\"Unique output clasess:\\n{y_counts}\")\n",
    "\n",
    "# Encode labels to integer categories\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e0ccb70fefdc43ea196e5bed1dad258852fcc07a4658b68f0531d3356971284"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
