{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0af1589",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Predicting site from fMRI: cross-validation and dimensionality reduction.\n",
    "\n",
    "Here we consider the same data and prediction task as in the tutorial of\n",
    "Machine Learning part 1 of the QLS course.\n",
    "\n",
    "We have some fMRI time series, that we use to compute a connectivity matrix\n",
    "for each participant. We use the connectivity matrix values as our input\n",
    "features to predict to which site the participant belongs.\n",
    "\n",
    "As in Part 1, we classify participants using a logistic regression. However\n",
    "we make several additions.\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "We use scikit-learn's `sklearn.pipeline.Pipeline`, that enables chaining\n",
    "several transformations into a single scikit-learn estimator (an object with\n",
    "a `fit` method). This avoids dealing with the connectivity feature extraction\n",
    "separately and ensures everything is fitted on the training data only --\n",
    "which is crucial here because we will add scaling a dimensionality reduction\n",
    "step with Principal Component Analysis.\n",
    "\n",
    "## Scaling\n",
    "\n",
    "We add scaling of the input features using scikit-learn's StandardScaler,\n",
    "which removes the mean and scales the features to unit variance. This helps\n",
    "the logistic regression solver converge faster and often improves\n",
    "performance.\n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "We also consider a pipeline that reduces the dimension of input features with\n",
    "PCA, and compare it to the baseline logistic regrssion. One advantage is that\n",
    "the pipeline that uses PCA can be fitted much faster.\n",
    "\n",
    "## Cross-validation\n",
    "\n",
    "In part 1 we fitted one model and evaluated it on a held-out test set. Here,\n",
    "we will use scikit-learn's `cross_validate` to perform K-Fold\n",
    "cross-validation and get a better estimate of our model's generalization\n",
    "performance. This allows comparing logistic regression with and without PCA,\n",
    "as well as a naive baseline.\n",
    "\n",
    "Moreover, instead of the plain `LogisticRegression`, we use scikit-learn's\n",
    "`LogisticRegressionCV`, which automatically performs a nested\n",
    "cross-validation loop on the training data to select the best hyperparameter.\n",
    "\n",
    "We therefore obtain a typical supervised learning experiment, with learning\n",
    "pipelines that involve chained transformations, hyperparameter selection, a\n",
    "cross-validation, and comparison of several models and a baseline.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Read, understand and run this script. `load_connectivity_data` loads the data\n",
    "and returns the matrices `X` and `y`. `prepare_pipelines` returns a\n",
    "dictionary whose values are scikit-learn estimators and whose keys are names\n",
    "for each estimator. All estimators are instances of scikit-learn's\n",
    "`Pipeline`, and the first step is always connectivity feature extraction with\n",
    "nilearn's `ConnectivityMeasure`.\n",
    "\n",
    "At the moment `prepare_pipelines` only returns 2 estimators: the logistic\n",
    "regression and a dummy estimator. Add a third estimator in the returned\n",
    "dictionary, which contains a dimensionality reduction step: a PCA with 20\n",
    "components. To do so, add a `sklearn.decomposition.PCA` as the second step of\n",
    "the pipeline. Note 20 is an arbitrary choice; how could we set the number of\n",
    "components in a principled way? What is the largest number of components we\n",
    "could ask for?\n",
    "Answer: include it in grid search, 80 (rank of X_train)\n",
    "\n",
    "There are 111 regions in the atlas we use to compute region-region\n",
    "connectivity matrices: the output of the `ConnectivityMeasure` has\n",
    "111 * (111 - 1) / 2 = 6105 columns. If the dataset has 100 participants, What\n",
    "is the size of the coefficients of the logistic regression? of the selected\n",
    "(20 first) principal components? of the output of the PCA transformation (ie\n",
    "the compressed design matrix)?\n",
    "Answer: 6105 coefficients + intercept; principal components: 20 x 6105;\n",
    "compressed X: 100 x 20.\n",
    "\n",
    "Here we are storing data and model coefficients in arrays of 64-bit\n",
    "floating-point values, meaning each number takes 64 bits = 8 bytes of memory.\n",
    "Approximately how much memory is used by the design matrix X? by the\n",
    "dimensionality-reduced data (ie the kept left singular vectors of X)? by the\n",
    "principal components (the kept right singular vectors of X)?\n",
    "Answer: X: 4,884,000 B, compressed X: 16,000 B, V: 976,800 B\n",
    "(+ 96 bytes for all for the array object)\n",
    "\n",
    "As you can see, in this script we do not specify explicitly the metric\n",
    "functions that are used to evaluate models, but rely on scikit-learn's\n",
    "defaults instead. What metric is used in order to select the best\n",
    "hyperparameter? What metric is used to compute scores in `cross_validate`?\n",
    "Are these defaults appropriate for our particular situation?\n",
    "Answer: sklearn.metrics.accuracy_score for both, yes\n",
    "\n",
    "We do not specify the cross-validation strategy either. Which\n",
    "cross-validation procedure is used in `cross_validate`, and by the\n",
    "`LogisticRegressionCV`? Are these choices appropriate?\n",
    "\n",
    "## Additional exercises (optional)\n",
    "\n",
    "Try replacing the default metrics with other scoring functions from\n",
    "scikit-learn or functions that you write yourself. Does the relative\n",
    "performance of the models change?\n",
    "\n",
    "Specify the cross-validation strategy explicitly, possibly choosing a\n",
    "different one than the default.\n",
    "\n",
    "Add another estimator to the options returned by `prepare_pipelines`, that\n",
    "uses univariate feature selection instead of PCA.\n",
    "\n",
    "What other approach could we use to obtain connectivity features of a lower\n",
    "dimension?\n",
    "Answer: use an atlas with less regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a556615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "from nilearn.connectome import ConnectivityMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f6046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386edee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e5d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_timeseries_and_site(n_subjects=100):\n",
    "    \"\"\"Load ABIDE timeseries and participants' site.\n",
    "\n",
    "    Returns X, a list with one array of shape (n_samples, n_rois) per\n",
    "    participant, and y, an array of length n_participants containing integers\n",
    "    representing the site each participant belongs to.\n",
    "\n",
    "    \"\"\"\n",
    "    data = datasets.fetch_abide_pcp(n_subjects=n_subjects, derivatives=[\"rois_ho\"])\n",
    "    X = data[\"rois_ho\"]\n",
    "    y = LabelEncoder().fit_transform(data[\"phenotypic\"][\"SITE_ID\"])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ebb4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_pipelines():\n",
    "    \"\"\"Prepare scikit-learn pipelines for fmri classification with connectivity.\n",
    "\n",
    "    Returns a dictionary where each value is a scikit-learn estimator (a\n",
    "    `Pipeline`) and the corresponding key is a descriptive string for that\n",
    "    estimator.\n",
    "\n",
    "    As an exercise you need to add a pipeline that performs dimensionality\n",
    "    reduction with PCA.\n",
    "\n",
    "    \"\"\"\n",
    "    connectivity = ConnectivityMeasure(kind=\"correlation\", vectorize=True, discard_diagonal=True)\n",
    "    scaling = StandardScaler()\n",
    "    logreg = LogisticRegressionCV(solver=\"liblinear\", cv=3, Cs=3)\n",
    "    logreg = LogisticRegression(C=10)\n",
    "    logistic_reg = make_pipeline(clone(connectivity), clone(scaling), clone(logreg))\n",
    "    # make_pipeline is a convenient way to create a Pipeline by passing the\n",
    "    # steps as arguments. clone creates a copy of the input estimator, to avoid\n",
    "    # sharing the state of an estimator across pipelines.\n",
    "    pca_logistic_reg = make_pipeline(\n",
    "        clone(connectivity),\n",
    "        clone(scaling),\n",
    "        PCA(n_components=20),\n",
    "        clone(logreg),\n",
    "    )\n",
    "    kbest_logistic_reg = make_pipeline(\n",
    "        clone(connectivity),\n",
    "        clone(scaling),\n",
    "        SelectKBest(f_classif, k=300),\n",
    "        clone(logreg),\n",
    "    )\n",
    "    dummy = make_pipeline(clone(connectivity), DummyClassifier())\n",
    "    # TODO: add a pipeline with a PCA dimensionality reduction step to this\n",
    "    # dictionary. You will need to import `sklearn.decomposition.PCA`.\n",
    "    return {\n",
    "        \"Logistic no PCA\": logistic_reg,\n",
    "        \"Logistic with PCA\": pca_logistic_reg,\n",
    "        \"Logistic with feature selection\": kbest_logistic_reg,\n",
    "        \"Dummy\": dummy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f1696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cv_scores(models, X, y):\n",
    "    \"\"\"Compute cross-validation scores for all models\n",
    "\n",
    "    `models` is a dictionary like the one returned by `prepare_pipelines`, ie\n",
    "    of the form `{\"model_name\": estimator}`, where `estimator` is a\n",
    "    scikit-learn estimator.\n",
    "\n",
    "    `X` and `y` are the design matrix and the outputs to predict.\n",
    "\n",
    "    Returns a `pd.DataFrame` with one row for each model and cross-validation\n",
    "    fold. Columns include `test_score` and `fit_time`.\n",
    "\n",
    "    \"\"\"\n",
    "    all_scores = []\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Computing scores for model: '{model_name}'\")\n",
    "        model_scores = pd.DataFrame(cross_validate(model, X, y, return_train_score=True))\n",
    "        model_scores[\"model\"] = model_name\n",
    "        all_scores.append(model_scores)\n",
    "    all_scores = pd.concat(all_scores)\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X, y = load_timeseries_and_site()\n",
    "    models = prepare_pipelines()\n",
    "    all_scores = compute_cv_scores(models, X, y)\n",
    "    print(all_scores.groupby(\"model\").mean())\n",
    "    sns.stripplot(data=all_scores, x=\"train_score\", y=\"model\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
