* export options                                                   :noexport:
** general
   #+STARTUP: beamer
   #+OPTIONS: H:3 toc:nil num:t date:nil

   #+LaTeX_CLASS: beamer
   #+LaTeX_CLASS_OPTIONS: [presentation,mathserif,table]

** macros
#+MACRO: course_url https://github.com/neurodatascience/QLS-course-materials/tree/main/Lectures/09-Machine_Learning_2/
#+MACRO: exercise [[https://github.com/neurodatascience/QLS-course-materials/blob/main/Lectures/09-Machine_Learning_2/exercises/$1][=$1=]]
** presentation info
   #+TITLE: Model selection and validation
   # #+AUTHOR: Jérôme Dockès

   #+BEAMER_HEADER: \author{Jérôme Dockès \& Nikhil Bhagwat}
   #+BEAMER_HEADER: \titlegraphic{\includegraphics[height=1.5cm]{figures/mcgill-university.png} \hspace{1.5cm} \includegraphics[height=1.5cm]{figures/origami-lab-logo.png}}
   #+BEAMER_HEADER: \date{QLS 612 2023-05-04}

** latex headers
*** fonts and beamer
    #+LaTeX_HEADER: \beamertemplatenavigationsymbolsempty

    #+LaTeX_HEADER: \usepackage[T1]{fontenc}

    #+LaTeX_HEADER: \usepackage{DejaVuSans}
    #+LaTeX_HEADER: \usepackage{DejaVuSansMono}

    # #+LaTeX_HEADER: \usepackage[default]{opensans}
    # #+LaTeX_HEADER: \usepackage{lmodern}
    # #+LaTeX_HEADER: \usepackage{libertine}
    # #+LaTeX_HEADER: \usepackage{iwona}
    # #+LaTeX_HEADER: \usepackage[sc,osf]{mathpazo}
    # #+LaTeX_HEADER: \usepackage{mathptmx}
    # #+LaTeX_HEADER: \usepackage{helvet}
    # #+LaTeX_HEADER: \usefonttheme{default}

    # #+LaTeX_HEADER: \usefonttheme{serif}
    #+LaTeX_HEADER: \usefonttheme{professionalfonts}

    #+LaTeX_HEADER: \usepackage[euler-digits,euler-hat-accent]{eulervm}

    # #+LaTeX_HEADER: \setbeamertemplate{itemize items}[circle]
    #+LaTeX_HEADER: \setbeamertemplate{itemize items}{•}
    #+LaTeX_HEADER: \setbeamertemplate{enumerate items}[default]

    #+LaTex_HEADER: \AtBeginSection[]
    #+LaTex_HEADER: {
    #+LaTex_HEADER: \begin{frame}<beamer>
    #+LaTex_HEADER: \frametitle{Outline}
    #+LaTex_HEADER: \tableofcontents[currentsection]
    #+LaTex_HEADER: \end{frame}
    #+LaTex_HEADER: }
    #+LaTex_HEADER: \setcounter{tocdepth}{1}

    #+LaTeX_HEADER: \setbeamertemplate{headline}{}
    #+LaTeX_HEADER: \setbeamertemplate{footline}{
    #+LaTeX_HEADER: \leavevmode%
    #+LaTeX_HEADER: \hbox{%
    #+LaTeX_HEADER: \begin{beamercolorbox}[wd=\paperwidth,ht=2.25ex,dp=1ex,right]{fg=black}%
    #+LaTeX_HEADER:     \usebeamerfont{section in head/foot}\insertsection\hspace*{2em}
    #+LaTeX_HEADER:     \insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
    #+LaTeX_HEADER: \end{beamercolorbox}%
    #+LaTeX_HEADER: }%
    #+LaTeX_HEADER: \vskip0pt%
    #+LaTeX_HEADER: }
    #+LaTeX_HEADER: \usepackage{appendixnumberbeamer}

    #+LaTeX_HEADER: \setbeamersize{text margin left=3mm,text margin right=3mm}
*** footnote citations
    #+LaTeX_HEADER: \newcommand\blfootnote[1]{%
    #+LaTeX_HEADER: \begingroup
    #+LaTeX_HEADER: \renewcommand\thefootnote{}\footnote{#1}%
    #+LaTeX_HEADER: \addtocounter{footnote}{-1}%
    #+LaTeX_HEADER:  \endgroup
    #+LaTeX_HEADER: }
    #+LaTeX_HEADER: \setbeamerfont{footnote}{size=\tiny}
*** other imports
    #+LaTeX_HEADER: \usepackage{tikz}
    #+LaTeX_HEADER: \usepackage[retainorgcmds]{IEEEtrantools}
    #+LaTeX_HEADER: \hypersetup{colorlinks=true, allcolors=., urlcolor=blue}
    #+LaTeX_HEADER: \usepackage[absolute,overlay]{textpos}

    #+LaTeX_HEADER: \usepackage{xcolor}
    #+LaTeX_HEADER: \definecolor{LightGray}{gray}{0.96}
    #+LaTeX_HEADER: \usepackage{minted}
    #+LaTeX_HEADER: \setminted{bgcolor=LightGray, fontsize=\small}


*** math operators
    #+LaTex_HEADER: \newcommand{\eg}{e.g.\,}
    #+LaTex_HEADER: \newcommand{\ie}{i.e.\,}
    #+LaTex_HEADER: \newcommand{\aka}{a.k.a.\,}
    #+LaTex_HEADER: \newcommand{\etc}{\emph{etc.}\,}

    #+LaTex_HEADER: \newcommand{\X}{{\mathbold X}}
    #+LaTex_HEADER: \newcommand{\bS}{{\mathbold S}}
    #+LaTex_HEADER: \newcommand{\bSigma}{{\mathbold \Sigma}}
    #+LaTex_HEADER: \newcommand{\x}{{\mathbold x}}
    #+LaTex_HEADER: \newcommand{\bbeta}{{\mathbold \beta}}
    #+LaTex_HEADER: \newcommand{\Y}{{\mathbold Y}}
    #+LaTex_HEADER: \newcommand{\y}{{\mathbold y}}
    #+LaTex_HEADER: \newcommand{\B}{{\mathbold B}}
    #+LaTex_HEADER: \newcommand{\W}{{\mathbold W}}
    #+LaTex_HEADER: \newcommand{\U}{{\mathbold U}}
    #+LaTex_HEADER: \newcommand{\V}{{\mathbold V}}
    #+LaTex_HEADER: \newcommand{\bH}{{\mathbold H}}
    #+LaTex_HEADER: \newcommand{\R}{\mathbb{R}}
    #+LaTex_HEADER: \DeclareMathOperator*{\argmin}{argmin}
    #+LaTex_HEADER: \DeclareMathOperator*{\argmax}{argmax}
    #+LaTex_HEADER: \DeclareMathOperator*{\tv}{TV}
    #+LaTex_HEADER: \DeclareMathOperator*{\Tr}{Tr}
    #+LaTex_HEADER: \DeclareMathOperator*{\FFT}{FFT}
    #+LaTex_HEADER: \DeclareMathOperator*{\IFFT}{IFFT}
    #+LaTex_HEADER: \DeclareMathOperator*{\diag}{diag}
    #+LaTex_HEADER: \DeclareMathOperator*{\supp}{supp}
    #+LaTex_HEADER: \DeclareMathOperator*{\tf}{tf}
    #+LaTex_HEADER: \DeclareMathOperator*{\idf}{idf}
    #+LaTex_HEADER: \DeclareMathOperator*{\df}{df}
    #+LaTex_HEADER: \DeclareMathOperator*{\Var}{Var}
    #+LaTex_HEADER: \DeclareMathOperator*{\Frob}{Frob}
    #+LaTex_HEADER: \DeclareMathOperator*{\F}{F}
    #+LaTex_HEADER: \DeclareMathOperator*{\softmax}{softmax}
    #+LaTex_HEADER: \DeclareMathOperator*{\AUC}{AUC}

    #+LaTeX_HEADER: \usepackage{bm}

** color theme
   # #+BEAMER_COLOR_THEME: dove
   # #+BEAMER_COLOR_THEME: seagull

   #+LaTeX_HEADER: \usecolortheme{dove}
   #+LaTeX_HEADER: \setbeamercolor*{block title example}{fg=black,bg=white}
   #+LaTeX_HEADER: \setbeamercolor*{block body example}{fg=black,bg=white}
* Introduction: cross-validation
** Recap of part 1
*** Recap of part 1
**** Supervised learning
       - Regression: least-squares linear regression
       - Classification: logistic regression
#+ATTR_LATEX: :height .4 \textheight :center
[[file:figures/generated/linear_regression_1d/linear_regression.pdf]]
#+ATTR_LATEX: :height .4 \textheight :center
[[file:figures/generated/logistic_regression_1d/logistic_regression.pdf]]

*** Recap of part 1
**** Supervised learning
     :PROPERTIES:
     :BEAMER_act: <1->
     :END:
       - Regression: least-squares linear regression
       - Classification: logistic regression
**** Regularization
     :PROPERTIES:
     :BEAMER_act: <1->
     :END:
       - \(\ell_2\) \aka ridge regularization
**** Model evaluation and selection
     :PROPERTIES:
     :BEAMER_act: <2->
     :END:
       - Out-of-sample generalization; independent test set
       - Performance metrics:
         - regression: mean squared error
         - classification: accuracy, ROC curve
       - Cross-validation
*** Notation & vocabulary
**** Supervised learning framework
 \begin{equation}
 Y = f(X) + E
 \end{equation}
\vspace{-10pt}
#+ATTR_BEAMER: :overlay +-
 - \(Y \in \R \): output (\aka target, dependent variable) to predict
 - \(X \in \R^p \): features (\aka inputs, regressors, descriptors, independent variables)
 - \(E \in \R \): unmodelled noise
 - \(f\): the function we try to approximate
**** Linear regression                                            :B_example:
     :PROPERTIES:
     :BEAMER_act: <4->
     :BEAMER_env: example
     :END:
\vspace{-20pt}
 \begin{IEEEeqnarray}{rCl}
 Y & = & \beta_0 + \langle X, \beta \rangle + E \\
& = & \beta_0 + \sum_{j=1}^p X_j \, \beta_j + E
 \end{IEEEeqnarray}
"learning" = choosing \(\beta_0 \in \R\) and \(\bbeta \in \R^p\)
*** How to set parameters: Empirical Risk Minimization
    - Choose a loss function \(L\) measuring how bad is our error.
    - Example: squared error \(L(Y, \hat{Y}) = (Y - \hat{Y})^2\), where \(\hat{Y}\) is the prediction
    - We want to minimize the expected error (risk): \(\mathbb{E}[L(Y, \hat{Y})]\)
*** How to set parameters: Empirical Risk Minimization
We do not know the risk: estimate it from a sample.

Given \(n\) training examples \(\X \in \R^{n \times p}\), \(\y \in \R^n\),
minimize the empirical risk: \(\sum_{i=1}^n L(\y_i, \hat{\y_i})\)

**** For linear regression:
find \(\hat{\beta}_0 \in \R, \hat{\bbeta} \in \R^p\) that minimize
\begin{IEEEeqnarray}{rcl}
\| \y - \hat{\y} \|_2^2 & \; = \; & \| \y - \hat{\beta}_0 - \X \, \hat{\bbeta} \|_2^2 \\
& \; = \; & \sum_{i=1}^n (\y_i - \hat{\beta}_0 - \sum_{j=1}^p \X_{ij}\, \hat{\bbeta}_j )^2
\end{IEEEeqnarray}

"Fitting" the parameters to \(\X, \y\).

*** Evaluating a model

**** We always want to do 2 distinct things:
  - Select a model (set the parameters).
  - Evaluate its performance.

\vfill

**** warning                                                 :B_structureenv:
     :PROPERTIES:
     :BEAMER_env: structureenv
     :END:
  We can never do both on the same data!
*** Training error is a biased estimator of the risk
- 30 different animals made predictions about match results in the 2022 World Cup
#+ATTR_LATEX: :height .65 \textheight
[[file:figures/generated/select_evaluate/select_evaluate_1.pdf]]
*** Training error is a biased estimator of the risk
- 30 different animals made predictions about match results in the 2022 world cup
- We *selected the best predictor* (highest accuracy)
#+ATTR_LATEX: :height .65 \textheight
[[file:figures/generated/select_evaluate/select_evaluate_2.pdf]]
*** Training error is a biased estimator of the risk
The same animal is invited to make predictions about the 2026 World Cup.
Is it more likely to perform:
1. Better than in 2022?
2. Worse than in 2022?
3. The same?
#+ATTR_LATEX: :height .65 \textheight
[[file:figures/generated/select_evaluate/select_evaluate_2b.pdf]]

*** Training error is a biased estimator of the risk
The selected animal is more likely to perform *worse* than in 2022.
#+ATTR_LATEX: :height .65 \textheight
[[file:figures/generated/select_evaluate/select_evaluate_3.pdf]]
*** Training error is a biased estimator of the risk
The selected animal is more likely to perform *worse* than in 2022.
#+ATTR_LATEX: :height .65 \textheight
[[file:figures/generated/select_evaluate/select_evaluate_3b.pdf]]
Its 2022 performance is a *biased* estimator of its expected performance in future World Cups.
*** Training error is a biased estimator of the risk
Distribution of train and test errors across 50 repetitions:
#+ATTR_LATEX: :height .5 \textheight
[[file:figures/generated/select_evaluate_averaged/select_evaluate_averaged_1.pdf]]
*** Training error is a biased estimator of the risk
- The systematic difference is the bias.
- It is why we cannot use the training error to estimate model performance.
#+ATTR_LATEX: :height .5 \textheight
[[file:figures/generated/select_evaluate_averaged/select_evaluate_averaged_2.pdf]]


*** Estimating prediction performance
When you hear "best", "maximum", "select", ... think "bias"
**** Setting the parameters
     - *Select* \(\bbeta\) that gives the *best* prediction on training data
     - The prediction score for \(\hat{\bbeta}\) is biased: compute a new score on unseen test data.
** Supervised learning with sklearn
*** scikit-learn "estimator API": =fit; predict=
  #+BEGIN_SRC python
  estimator = Ridge()
  estimator.fit(X_train, y_train)
  predictions = estimator.predict(X_test)
  #+END_SRC
  \vfill
  [[https://scikit-learn.org/stable/getting_started.html][Scikit-learn user guide]]

  [[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html][=sklearn.linear_model.Ridge=]]

\vfill

("API": "Application Programming Interface" -- the specific way in which the library exposes its behaviour to user code: method names & signatures, etc.)
*** Evaluating performance with =sklearn.metrics=
  #+BEGIN_SRC python
  estimator = Ridge()
  estimator.fit(X_train, y_train)
  predictions = estimator.predict(X_test)

  mse = metrics.mean_squared_error(y_test, predictions)
  #+END_SRC
  \vfill

  [[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html][=sklearn.linear_model.Ridge=]]

  [[https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics][=sklearn.metrics=]]

  [[https://scikit-learn.org/stable/modules/model_evaluation.html][User guide on model evaluation]]
  \vfill
  {{{exercise(ex_01_fit_predict_questions.py)}}}

*** Some possible metrics for regression

**** \(R^2\) score (coefficient of determination): [[https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score-the-coefficient-of-determination][=r2_score=]]
\begin{equation}
R^2(\y, \hat{\y}) = 1 - \frac{\sum_{i=1}^n(y_i  - \hat{y}_i)^2}{\sum_{i=1}^n(y_i  - \bar{y})^2} \; ,
\end{equation}
where \(\bar{y} = \frac{1}{n}\sum_{i=1}^n y_i\)

**** Mean Squared Error (MSE): [[https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error][=mean_squared_error=]]
\begin{equation}
\text{MSE}(\y, \hat{\y}) = \frac{1}{n} \sum_{i=1}^n(y_i  - \hat{y}_i)^2
\end{equation}

**** Mean Absolute Error (MAE): [[https://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-error][=mean_absolute_error=]]
\begin{equation}
\text{MAE}(\y, \hat{\y}) = \frac{1}{n} \sum_{i=1}^n |y_i  - \hat{y}_i|
\end{equation}

** cv
*** Cross-validation
  #+ATTR_LATEX: :height .7 \textheight
  [[file:figures/generated/cv_figure_simple.pdf]]

\vspace{-10pt}

  [[https://scikit-learn.org/stable/modules/cross_validation.html][User guide on cross-validation]]
  [[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html][=sklearn.model_selection.cross_validate=]]
  [[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html][=sklearn.model_selection.cross_val_score=]]
  {{{exercise(ex_02_cross_validate_questions.py)}}}
* Model and hyperparameter selection
** nested cv
*** Need for regularization
Linear regression: projection on the column space of \(X\)
\vspace{10pt}
**** top                                                     :B_structureenv:
     :PROPERTIES:
     :BEAMER_env: structureenv
     :END:
***** equation                                               :B_column:BMCOL:
      :PROPERTIES:
      :BEAMER_env: column
      :BEAMER_col: .3
      :END:
      \begin{equation}
      \hat{\y} = \X \, \hat{\bbeta}
      \end{equation}

***** equation                                               :B_column:BMCOL:
      :PROPERTIES:
      :BEAMER_env: column
      :BEAMER_col: .7
      :END:
      \vspace{-17pt}
      #+ATTR_LATEX: :height .7\textheight
      [[file:figures/generated/dim_reduction_colors/regression_full_3.pdf]]

**** bottom                                                  :B_structureenv:
     :PROPERTIES:
     :BEAMER_env: structureenv
     :END:
     - Too many features: high variance & unstable solution
     - Solutions: *regularization*, dimensionality reduction
*** Regularization

**** Ridge regression                                             :B_example:
     :PROPERTIES:
     :BEAMER_env: example
     :END:
\begin{equation}
\argmin_{\bbeta, \beta_0} \| \y - \beta_0 - \X \, \bbeta \|_2^2 + \alpha \, \|\bbeta\|_2^2
\end{equation}
*** Example hyperparameter: regularization                      :B_fullframe:
    :PROPERTIES:
    :BEAMER_env: fullframe
    :END:
**** var                                                              :BMCOL:
     :PROPERTIES:
     :BEAMER_col: .33
     :END:
  \(\small{ \text{Var}(\hat{\beta}_i) = \mathbb{E}(\hat{\beta}_i  - \mathbb{E}(\hat{\beta}_i))^2} \)

**** plot                                                             :BMCOL:
     :PROPERTIES:
     :BEAMER_col: .38
     :END:
\vspace{-15pt}
     #+ATTR_LATEX: :height \textheight
     [[file:figures/generated/ridge_regularization_path/ridge_regularization_path.pdf]]
**** bias                                                             :BMCOL:
     :PROPERTIES:
     :BEAMER_col: .3
     :END:
  \(\small \text{Bias}(\hat{\beta}_i) = \mathbb{E}(\hat{\beta}_i) - \beta_i\)

*** Setting hyperparameters
**** How can we choose the ridge hyperparameter \(\alpha\)?
**** answer                                                 :B_ignoreheading:
     :PROPERTIES:
     :BEAMER_env: ignoreheading
     :END:
     Try a few and pick the best one...

     But measure its performance on separate data!
*** Nested cross-validation
When you hear "best", "maximum", "select", ... think "bias"
**** Setting the parameters
    :PROPERTIES:
    :BEAMER_act: <2->
    :END:
     - *Select* \(\bbeta\) that gives the *best* prediction on training data
     - The prediction score for \(\hat{\bbeta}\) is biased: compute a new score on unseen test data.
**** Setting the hyperparameters
    :PROPERTIES:
    :BEAMER_act: <3->
    :END:
     - Repeat step 1 for a few values of \(\alpha\), fitting and testing several models
     - *Select* the hyperparameter that obtains the *best* prediction on test data
     - The prediction score of that model on /test/ data is biased: evaluate it again on unseen data
*** One split
[[file:figures/generated/train_eval_test/datasets.pdf]]
*** Nested cross-validation
[[file:figures/generated/cv_figure_nested.pdf]]
  see  [[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html][=sklearn.model_selection.GridSearchCV=]]

*** Nested cross-validation with scikit-learn
- In general: [[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html][=GridSearchCV=]] ([[https://scikit-learn.org/stable/modules/grid_search.html#grid-search][User Guide]])
\vfill
#+BEGIN_SRC python
model = GridSearchCV(
    Ridge(), {"alpha": [.1, 1., 10.]})
scores = cross_val_score(model, X, y)
#+END_SRC
\vfill
- Use [[https://scikit-learn.org/stable/glossary.html#term-cross-validation-estimator][CV estimators]] when possible: [[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html][=RidgeCV=]], [[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html][=LassoCV=]], ...

\vfill

{{{exercise(ex_03_grid_search_regression_questions.py)}}}
*** Implementing nested CV
{{{exercise(ex_04_nested_cross_validation_questions.py)}}}
* Dimensionality reduction
** Intro
*** Dimensionality reduction
Linear regression: projection on the column space of \(X\)
\vspace{10pt}
**** top                                                     :B_structureenv:
     :PROPERTIES:
     :BEAMER_env: structureenv
     :END:
***** equation                                               :B_column:BMCOL:
      :PROPERTIES:
      :BEAMER_env: column
      :BEAMER_col: .3
      :END:
      \begin{equation}
      \hat{\y} = \X \, \hat{\bbeta}
      \end{equation}

***** equation                                               :B_column:BMCOL:
      :PROPERTIES:
      :BEAMER_env: column
      :BEAMER_col: .7
      :END:
      \vspace{-17pt}
      #+ATTR_LATEX: :height .7\textheight
      [[file:figures/generated/dim_reduction_colors/regression_full_3.pdf]]

**** bottom                                                  :B_structureenv:
     :PROPERTIES:
     :BEAMER_env: structureenv
     :END:
     - Too many features: high variance & unstable solution
     - Solutions: regularization, *dimensionality reduction*
*** Dimensionality reduction
**** Until now
     #+ATTR_LATEX: :height .12 \textheight
     [[file:figures/graphs/pipeline-1.pdf]]
**** Add a step in the pipeline: simplifying the inputs
     #+ATTR_LATEX: :height .12 \textheight
     [[file:figures/graphs/pipeline-2.pdf]]
*** Simulated data for linear regression
    - Generate \(\X \in \R^{n \times 3}\), \(\mathbold{\bbeta} \in \R^3\), \(\mathbold{e} \in \R^n\) and \(\y = \X \, \bbeta + \mathbold{e} \in R^n\)
    - Append columns containing random noise to \(\X\)
    - Now \(\X \in \R^{n \times p}\), with \(p \geq 3\), but only the first 3 columns are linked with \(\y\)
    - Split into training and testing tests and evaluate a linear regression model: what happens when \(p\) becomes large?
  # \vfill

See [[https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression][=sklearn.datasets.make_regression=]] for generating data
#+ATTR_LATEX: :height .4 \textheight
[[file:figures/generated/show_make_regression/x_construction.pdf]]
*** Model complexity: overfitting
    - Model complexity increases with dimension.
    - Example: a linear model in dimension \(p\) can fit exactly (0 training error) any set of \(p + 1\) points.
    - Risk of overfitting: fitting exactly training data but failing on test data

    #+ATTR_LATEX: :height .7\textheight
    [[file:figures/generated/ridge_overfitting/mse_log.pdf]]
** Univariate feature selection
*** Univariate feature selection
    - \aka feature screening, filtering ...
    - Check features (columns of \(\X\)) one by one for association with the output \(\y\)
    - Keep only a fixed number or percentage of the features

**** Simple (linear) association criteria
     - for regression: correlation
     - for classification: ANalysis Of VAriance
**** Read more in the scikit-learn user guide
     [[https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection][scikit-learn feature selection]]

*** Original regression problem
***** equation                                               :B_column:BMCOL:
      :PROPERTIES:
      :BEAMER_env: column
      :BEAMER_col: .3
      :END:
      \begin{equation}
      \hat{\y} = \X \, \hat{\bbeta}
      \end{equation}

***** equation                                               :B_column:BMCOL:
      :PROPERTIES:
      :BEAMER_env: column
      :BEAMER_col: .7
      :END:
      \vspace{-17pt}
      #+ATTR_LATEX: :height .7\textheight
[[file:figures/generated/dim_reduction_colors/regression_full_3.pdf]]
*** After univariate feature selection
***** equation                                               :B_column:BMCOL:
      :PROPERTIES:
      :BEAMER_env: column
      :BEAMER_col: .3
      :END:
      \begin{equation}
      \hat{\y} = \X \, \hat{\bbeta}
      \end{equation}

***** equation                                               :B_column:BMCOL:
      :PROPERTIES:
      :BEAMER_env: column
      :BEAMER_col: .7
      :END:
      \vspace{-17pt}
      #+ATTR_LATEX: :height .7\textheight
      [[file:figures/generated/feature_selection_colors/regression_selected_3_full_coef.pdf]]

*** After univariate feature selection
***** equation                                               :B_column:BMCOL:
      :PROPERTIES:
      :BEAMER_env: column
      :BEAMER_col: .3
      :END:
      \begin{equation}
      \hat{\y} = \X \, \hat{\bbeta}
      \end{equation}

***** equation                                               :B_column:BMCOL:
      :PROPERTIES:
      :BEAMER_env: column
      :BEAMER_col: .7
      :END:
      \vspace{-17pt}
      #+ATTR_LATEX: :height .7\textheight
      [[file:figures/generated/feature_selection_colors/regression_selected_3.pdf]]

*** Univariate feature selection
    Keeping only the 10 best features (most correlated with \(\y\))
    #+ATTR_LATEX: :height .7\textheight
    [[file:figures/generated/ridge_overfitting/mse_with_dim_reduction_log.pdf]]

** Fit whole pipeline on train data only
*** Dataset transformations
**** Typical pipeline
[[file:figures/graphs/pipeline-2-no-color.pdf]]
**** Example
[[file:figures/graphs/pipeline-3.pdf]]
*** scikit-learn "transformer API": =fit; transform=
    #+BEGIN_SRC python
  transformer = SelectKBest()
  transformer.fit(X_train, y_train)
  transformed_train = transformer.transform(X_train)
    #+END_SRC
**** can also be written:
     #+BEGIN_SRC python
       transformer = SelectKBest()
       transformed_train = transformer.fit_transform(
           X_train, y_train)
     #+END_SRC
**** links                                                   :B_structureenv:
     :PROPERTIES:
     :BEAMER_env: structureenv
     :END:
   \vfill

   [[https://scikit-learn.org/stable/modules/feature_selection.html][scikit-learn feature selection]]

[[https://scikit-learn.org/stable/getting_started.html#transformers-and-pre-processors][scikit-learn =Transformer= API]]
  \vfill

*** =feature_selection.SelectKBest=
**** =fit:=
     - compute ANOVA or correlation for each column of \(X\)
     - Remember the indices of the \(k\) columns with highest scores
**** =transform:=
     - Index input to keep only the \(k\) selected columns


**** link                                                    :B_structureenv:
     :PROPERTIES:
     :BEAMER_env: structureenv
     :END:
  [[https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest][=sklearn.feature_selection.SelectKBest=]]



*** Fit the transformer only on train data!
    #+BEGIN_SRC python
      transformer = SelectKBest()
      transformed_train = transformer.fit_transform(
          X_train, y_train)

      transformed_test = transformer.transform(X_test)
    #+END_SRC

*** Pipelines
To chain transformations and an estimator, use [[https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html][=sklearn.pipeline.Pipeline=]]

- can be used to properly cross-validate whole pipeline
- can be combined with =cross_validate=, =GridSearchCV=, ...
- easily created with [[https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html][=sklearn.pipeline.make_pipeline=]]

#+BEGIN_SRC python
model = make_pipeline(SelectKBest(), Ridge())
#+END_SRC
**** links                                                   :B_structureenv:
     :PROPERTIES:
     :BEAMER_env: structureenv
     :END:
 \vfill
 {{{exercise(ex_05_feature_selection_questions.py)}}}
** Linear decomposition methods
*** Linear decomposition methods
Another approach to dimensionality reduction
**** Maybe OK to drop $\X_2$:
     \vspace{-10pt}
     #+ATTR_LATEX: :height .3\textheight
     [[file:figures/generated/pca/cloud_aligned.pdf]]
     \vspace{-20pt}
**** Data low-dimensional but no feature can be dropped:
     #+ATTR_LATEX: :height .3\textheight
     [[file:figures/generated/pca/cloud_not_aligned.pdf]]

Find a better referential in which to represent the data
*** COMMENT Linear regression: projection on the column space of \(\X\)
**** Approximate \(y\) as a combination of the columns of \(X\)
  \begin{equation}
  \hat{\y} = \X \, \hat{\bbeta} \in \R^n
  \end{equation}
- The columns of \(X\) are a family of \(p\) \(n\)-dimensional vectors
- When \(p\) is high or the columns of \(X\) are correlated, we want to use a family of \(k < p\) instead
- Feature selection: drop some columns, keep only \(k\)
- Could we build a better family of \(k\) vectors?
*** Linear regression: projection on the column space of \(X\)
**** top                                                     :B_structureenv:
     :PROPERTIES:
     :BEAMER_env: structureenv
     :END:
***** equation                                               :B_column:BMCOL:
      :PROPERTIES:
      :BEAMER_env: column
      :BEAMER_col: .3
      :END:
      \begin{equation}
      \hat{\y} = \X \, \hat{\bbeta}
      \end{equation}

***** equation                                               :B_column:BMCOL:
      :PROPERTIES:
      :BEAMER_env: column
      :BEAMER_col: .7
      :END:
      \vspace{-17pt}
      #+ATTR_LATEX: :height .7\textheight
      [[file:figures/generated/dim_reduction_colors/regression_full_3.pdf]]

**** bottom                                                  :B_structureenv:
     :PROPERTIES:
     :BEAMER_env: structureenv
     :END:
     - Too many features: high variance & unstable solution
     - Feature selection: drop some columns of \(\X\)
     - Other ways to build a family of \(k\) vectors on which to regress \(\y\)?
*** Linear decomposition: low-rank approximation of \(\X\)
    Minimize
\begin{equation}
\| \X - \W \, \bH \|_{\F}^2 = \sum_{i, j} ( \X_{i,j} - (\W \, \bH)_{i,j})^2
\end{equation}
    #+ATTR_LATEX: :height .5\textheight
    [[file:figures/generated/dim_reduction_colors/factorization_3.pdf]]
*** Linear regression after dimensionality reduction
    \begin{equation}
    \hat{\y} = \W \, \hat{\bbeta}
    \end{equation}
    #+ATTR_LATEX: :height .7\textheight
    [[file:figures/generated/dim_reduction_colors/regression_reduced_3.pdf]]
*** Prediction for a new data point \(\x \in \R^{p}\)
    - Find the combination of rows of \(\bH\) that is closest to \(\x\): regress \(\x\) on \(\bH^T\)
    - Multiply by \(\hat{\bbeta}\)
    \begin{equation}
\x \in \R^p \rightarrow \text{projection} \rightarrow \mathbold{w} \in \R^k \rightarrow \langle \cdot \, , \, \hat{\bbeta}\rangle \rightarrow \hat{y} \in \R
    \end{equation}
*** Principal Component Analysis
    - Singular Value Decomposition of \(\X\):
    \begin{equation}
    \X = \U \, \bS \, \V^T
    \end{equation}
    with \(\X \in \R^{n \times p}\), \(\U \in \R^{n \times r}\), \(\bS \in \R^{r \times r}\), \(\V \in \R^{r \times p}\)
    - \(r = \min(n, p)\)
    - \(\bS \succeq 0\) diagonal with decreasing values \(s_j\) along the diagonal
    - \(\U^T\, \U = I_r\)
    - \(\V^T\, \V = I_r\)

Truncating the SVD to keep only the first \(k\) components gives the best rank-\(k\) approximation of \(\X\)
#+ATTR_LATEX: :height .3\textheight
[[file:figures/generated/pca/cloud_not_aligned_with_pc.pdf]]
*** Singular Value Decomposition
\begin{equation}
\X = \U \, \bS \, \V^T
\end{equation}
#+ATTR_LATEX: :height .5 \textheight :center
[[file:figures/generated/pca_step_by_step/pca_steps_1.pdf]]

\begin{equation}
\U^T \, \U = I_p
\end{equation}
\begin{equation}
\V^T \, \V = I_p
\end{equation}

*** Singular Value Decomposition
\begin{equation}
\X = \U \, \bS \, \V^T
\end{equation}
#+ATTR_LATEX: :height .5 \textheight :center
[[file:figures/generated/pca_step_by_step/pca_steps_2.pdf]]

\begin{equation}
\U^T \, \U = I_p
\end{equation}
\begin{equation}
\V^T \, \V = I_p
\end{equation}


*** Singular Value Decomposition
\begin{equation}
\X = \U \, \bS \, \V^T
\end{equation}
#+ATTR_LATEX: :height .5 \textheight :center
[[file:figures/generated/pca_step_by_step/pca_steps_3.pdf]]

\begin{equation}
\U^T \, \U = I_p
\end{equation}
\begin{equation}
\V^T \, \V = I_p
\end{equation}

*** Other decomposition methods
Many other methods use the same objective (sum of squared reconstruction errors), but add penalties or constraints on the factors
- Dictionary Learning
- Non-negative Matrix Factorization
- K-means clustering
- ...

**** What about \(\y\)?
     - PCA is an example of /unsupervised/ learning: it does not use \(\y\)
     - Some other methods take it into account: \eg Partial Least Squares
*** Ridge regression and PCA
    - Both ridge regression and PC regression compute the coordinates of \(\y\) in the basis given by the SVD of \(\X\)
    - Ridge shrinks the coordinate along \(\U_j\) by a factor \(s_j^2 / (s_j^2 + \alpha)\)
    - PC regression sets the coordinates to 0 except for those corresponding to the \(k\) largest \(s_j\): shrinks by a factor \(\mathbold{1}_{\{j \leq k\}}\)

#+ATTR_LATEX: :height .6\textheight
[[file:figures/generated/dim_reduction_colors/regression_reduced_3_svd.pdf]]
* Conclusion: summary of pitfalls
*** (Cross-)validation experiments are simulations
The validation experiments must simulate what will happen when deploying the trained model in production -- when starting to use it in real life.
*** (Cross-)validation experiments are simulations
The validation experiments must simulate what will happen when deploying the trained model in production -- when starting to use it in real life.
**** Deploying a model to a hospital                             :B_example:
     :PROPERTIES:
     :BEAMER_env: example
     :END:
A model is trained on research dataset and then shipped and used on a hospital's patients.
We cannot:
- Preprocess the patients' data together with the training data.
- Use the patients' data for feature selection.
- Try different models on the patients' data and pick the best.

If we do any of these things in our cross-validation it is not a realistic experiment.
*** Split choice example: time series
Don't ignore dependencies between samples: which is easier?
#+ATTR_LATEX: :height .3 \textheight
[[file:figures/generated/time_series_cv/kfold.pdf]]

#+ATTR_LATEX: :height .3 \textheight
[[file:figures/generated/time_series_cv/kfold_shuffled.pdf]]

Use the appropriate [[https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators][cross-validation iterator]]
*** Remember that CV training sets overlap
    #+ATTR_LATEX: :height .6 \textheight
[[file:figures/generated/train_eval_test/cv_not_nested.pdf]]

So the scores are not independent! Their variance can be underestimated.

*** Some pitfalls with cross-validation
\small
**** Overfitting the hyperparameters
       + select hyperparameters with nested CV [[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html][=sklearn.model_selection.GridSearchCV=]]
**** Fitting part of the pipeline on the whole dataset
       + use  [[https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html][=sklearn.pipeline.Pipeline=]]
**** Ignoring dependencies between samples
         + e.g. time series: use appropriate [[https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators][cross-validation iterator]]
**** Ignoring dependencies between CV scores
         + Training sets overlap: cross-validation scores of different splits are not independent
**** Over-interpreting good CV scores
